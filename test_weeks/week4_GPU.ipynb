{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this work, you are required to train a model for dataset MNIST with PyTorch."
   ],
   "metadata": {
    "id": "chi4omZLZbt4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load dataset:"
   ],
   "metadata": {
    "id": "jrA70ruIaN-A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#need to set three item on GPU, such as the dataset (data and target), model, loss function  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:35:52.510690300Z",
     "start_time": "2024-05-17T09:35:52.444675300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dguqwltQ9TC4",
    "ExecuteTime": {
     "end_time": "2024-05-17T09:35:58.271517300Z",
     "start_time": "2024-05-17T09:35:52.458679200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchaudio import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#define the equipment to train\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "#load the dataset of mnist\n",
    "train_data = MNIST(\n",
    "    root = \"data\",\n",
    "    download = True,\n",
    "    train = True,\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    )\n",
    "test_data = MNIST(\n",
    "    root = \"data\",\n",
    "    download = True,\n",
    "    train = False,\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = 64)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
      "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
      "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#check the dimension\n",
    "examples = enumerate(train_dataloader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_targets)\n",
    "print(example_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:35:58.324708900Z",
     "start_time": "2024-05-17T09:35:58.266516100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13463666296\\AppData\\Local\\Temp\\ipykernel_29784\\2119449247.py:6: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout() #自动检查子图的坐标轴，标题和刻度标签\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 6 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAGkCAYAAACb5OmoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwsklEQVR4nO3dd3yUZb7+8WuSmB46UpWOSFfAxfKig4iIStFFOkgR27GwiLCQoKJG3bPowlmOCgpiR8ACKkIsNIX1sNhQwQVUkEAUAqFJMr8/+JHdON/HZJJJck/yeb9e/JFrnnnmHpg7XHkm99w+v9/vFwAAAEpdRGkPAAAAAKdRzAAAABxBMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwBMUMAADAERQzAAAAR5RqMdu6davGjBmjRo0aKS4uTnFxcWrSpInGjx+vzZs3l+bQiszn8yk5Odnz9i5dusjn8+X75/fOURBHjx5VcnKy3n///YDbkpOT5fP5dODAgSI9xm95PbfevXuH9HFgY16VzXklSe+9954uvvhixcfHq1q1aho5cqTS09ND/jgIxLwqu/PqjGPHjqlp06by+Xx69NFHi+1x8hNVWg88b9483XLLLTrvvPN0++23q0WLFvL5fPrqq6/0wgsvqEOHDtq+fbsaNWpUWkMsVnPnzlVmZmbu12+99Zbuv/9+LViwQM2aNcvN69atW6THOXr0qFJSUiSdnlwlpWHDhlq8eHGerFKlSiX2+OUV86rszqsPPvhAV1xxha688kotX75c6enpmjx5srp3767NmzcrJiamRMZRHjGvyu68+k9//vOflZWVVeKP+1ulUszWrVuniRMn6sorr9Srr76q6Ojo3Nu6deumm2++Wa+88ori4uJ+9zxHjx5VfHx8cQ+3WDRv3jzP19u2bZMktWzZUu3bt/e8X7g857i4OHXs2LG0h1GuMK/K9ryaNGmSmjZtqldffVVRUae/dTdo0ECXXnqp5s+fr5tuuqmUR1g2Ma/K9rw645NPPtETTzyhxYsXa9CgQaU6llJ5K3PWrFmKjIzUvHnz8rzI/9OgQYNUu3bt3K9HjhypxMREffbZZ+rVq5eSkpLUvXt3SdLPP/+siRMnqk6dOoqOjlbDhg01depUnThxIvf+O3fulM/n0zPPPBPwWL+9BHvmkukXX3yhwYMHq2LFiqpRo4ZGjx6tQ4cO5blvZmamxo4dq6pVqyoxMVG9e/fWN998U4S/nX87M45PP/1UAwcOVOXKlXN/IuvSpYv5E8XIkSNVv3793OdcvXp1SVJKSkru5eaRI0fmuc++ffvyfZ5wH/OqYMJxXv3444/atGmThg0bllvKJOmSSy5R06ZNtXTp0kKdF/ljXhVMOM6rM06ePKnRo0fr5ptv/t2iWVJKvJhlZ2crLS1N7du3V61atYK678mTJ9WvXz9169ZNy5cvV0pKio4fP66uXbtq4cKFuvPOO/XWW29p6NChSk1NVf/+/Ys01gEDBqhp06ZasmSJ7rnnHj3//PO64447cm/3+/265pprtGjRIt11111aunSpOnbsqCuuuKJIj/tb/fv3V+PGjfXKK6/o73//e4HvV6tWLb399tuSpDFjxmjDhg3asGGD/vznP+c5Lr/nKf170lnv/Vt27NihKlWqKCoqSo0aNdLUqVN17NixAo8dwWFeBS+c5tXnn38uSWrdunXAba1bt869HaHFvApeOM2rM2bOnKmsrCzdd999BR5vcSrxtzIPHDigY8eOqV69egG3ZWdny+/3534dGRkpn8+X+/Wvv/6q6dOna9SoUbnZvHnztHXrVr388su5lx979uypxMRETZ48WatWrVLPnj0LNdYxY8Zo0qRJkqQePXpo+/btmj9/vp5++mn5fD698847SktL0+zZs3XbbbflPnZ0dLSmTp1aqMe0jBgxIvd992DExMSoXbt2kk6/9+/11mJ+z1OSIiIiAv49vFx22WW6/vrr1axZMx07dkwrV65Uamqq1q5dq7S0NEVEsBg41JhXwQuneZWRkSFJqlKlSsBtVapUyb0docW8Cl44zStJ2rJli1JTU/XGG28oISFB+/fvD3rsoebU/5Dt2rXTWWedlfvnscceCzhmwIABeb5es2aNEhISNHDgwDz5mcufq1evLvR4+vXrl+fr1q1b6/jx47mroNLS0iRJQ4YMyXPcDTfcUOjHtPz2OYdafs9TkqZPn65Tp06pc+fO+Z7v/vvv10033aSuXbuqT58+euKJJ/TQQw/pww8/1PLly0M+fvw+5pUt3OaVJM//aAryHxBCi3llC6d5derUKY0ePVrXX3+9Lr/88mIZb2GUeDGrVq2a4uLitGvXroDbnn/+eW3atEmvv/66ed/4+HhVqFAhT5aRkaGaNWsGfGM6++yzFRUVVaSfJKtWrZrn6zOrns68JZeRkaGoqKiA42rWrFnox7QEewk9WPk9z1AYOnSoJGnjxo0hOyf+jXkVvHCaV2fOZf29//zzz+aVNBQd8yp44TSv/vrXv+q7777TjBkzdPDgQR08eDB39enx48d18OBBZWdnF33QQSrxYhYZGalu3bpp8+bN2rt3b57bmjdvrvbt26tVq1bmfa2fCqtWrap9+/bluaQsSenp6Tp16pSqVasmSYqNjZWkPL9gKdnf6AqqatWqOnXqVMA5fvrpp0Kf02I979jY2IDnIqlYP+MlFHgbs3gwr4IXTvOqZcuWkqTPPvss4LbPPvss93aEFvMqeOE0rz7//HMdOnRITZo0UeXKlVW5cmW1adNG0umPzqhcubI554pbqfwvOWXKFGVnZ2vChAn69ddfi3Su7t2768iRI1q2bFmefOHChbm3S1KNGjUUGxurrVu35jmuKG+tde3aVZICPq/r+eefL/Q5C6p+/fr65ptv8rzYMzIytH79+jzHFcfVr8J49tlnJYmP0ChGzKuic3Ve1alTRxdddJGee+65PD/Bb9y4UV9//XWRf3Ec3phXRefqvLrnnnuUlpaW588LL7wgSZowYYLS0tLUuHHjEhnLfyqVzzG79NJLNWfOHN1666268MILNW7cOLVo0UIRERHau3evlixZIkkBl4Etw4cP15w5czRixAjt3LlTrVq10tq1azVr1iz16dNHPXr0kHS6xQ8dOlTz589Xo0aN1KZNG33yySdFelH26tVLnTp10p/+9CdlZWWpffv2WrdunRYtWlTocxbUsGHDNG/ePA0dOlRjx45VRkaGUlNTA/7OkpKSVK9ePS1fvlzdu3dXlSpVVK1atdwlygU1c+ZMzZw5U6tXr/7d9+0/+ugjPfDAA7r22mvVsGFDHT9+XCtXrtT//u//qlu3brrqqqsK83RRAMyronN1XknSww8/rJ49e2rQoEGaOHGi0tPTdc8996hly5Z5fsEcocW8KjpX51WzZs3yfECudPpjOySpUaNGpfIht1IpfvL/hAkTdPHFF2v27Nn67//+b+3Zs0c+n09169bVJZdcotWrV6tbt275nic2NlZpaWmaOnWqHnnkEe3fv1916tTR3XffrRkzZuQ59swvZ6ampurIkSPq1q2b3nzzzaD/0c+IiIjQ66+/rjvvvFOpqak6efKkLr30Uq1YsSLgHzvULr30Uj377LN66KGHdPXVV6thw4aaMWOGVqxYEbBE+Omnn9akSZPUr18/nThxQiNGjDA/H+f35OTkBKxCstSqVUuRkZG67777dODAAfl8PjVp0kQzZ87UXXfdxVuZxYx5VTSuzivp9GdBrVixQtOnT9dVV12l+Ph49e3bV4888gif+l/MmFdF4/K8cpHPH64jBwAAKGO4fAEAAOAIihkAAIAjKGYAAACOoJgBAAA4gmIGAADgCIoZAACAIwr0OWY5OTnas2ePkpKS2CwXTvH7/Tp8+LBq164ddp+RxryCq5hXQOgVdF4VqJjt2bNH55xzTsgGB4Ta999/r7p165b2MILCvILrmFdA6OU3rwr0o1BSUlLIBgQUh3B8jYbjmFG+hONrNBzHjPIlv9dogYoZl4PhunB8jYbjmFG+hONrNBzHjPIlv9doeP3yAAAAQBlGMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwBMUMAADAERQzAAAAR1DMAAAAHEExAwAAcATFDAAAwBEUMwAAAEdQzAAAABxBMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwBMUMAADAERQzAAAAR1DMAAAAHBFV2gMAgOLUrl07M7/lllvMfPjw4Wa+cOFCM3/iiSfM/NNPPy3A6AAgL66YAQAAOIJiBgAA4AiKGQAAgCMoZgAAAI6gmAEAADiCVZkhEhkZaeYVK1Ys8rm9Vo/Fx8eb+XnnnWfmN998s5k/+uijZj548GAzP378uJk/9NBDZp6SkmLmQCi1bdvWzFetWmXmFSpUMHO/32/mw4YNM/N+/fqZedWqVc0cQOF1797dzBcvXmzmnTt3NvOvv/46ZGMKNa6YAQAAOIJiBgAA4AiKGQAAgCMoZgAAAI6gmAEAADii3KzKPPfcc808OjrazC+55BIzv+yyy8y8UqVKZj5gwID8BxdiP/zwg5k//vjjZn7ttdea+eHDh838n//8p5l/8MEHBRgdUHQXXXRRQLZkyRLzWK+V0V6rL71e9ydPnjRzr9WXHTt2NHOvPTS9zg/3dOrUycy9XgtLly4tzuGUKx06dDDzTZs2lfBIig9XzAAAABxBMQMAAHAExQwAAMARFDMAAABHUMwAAAAcUeZWZXrtl7dmzRozD8VelqUlJyfHzKdNm2bmR44cMXOvPcb27t1r5r/88ouZu7z3GNzmte/rhRdeaObPPfdcQFarVq2QjOXbb78189TUVDN/8cUXzXzdunVm7jU/H3zwwQKMDi7o0qWLmTdp0sTMWZUZvIgI+7pRgwYNzLxevXpm7vP5QjamksIVMwAAAEdQzAAAABxBMQMAAHAExQwAAMARFDMAAABHlLlVmbt37zbzjIwMMy+NVZkff/yxmR88eNDMu3btauZee+stWrSoUOMCSsu8efPMfPDgwSU8Eu+VoImJiWbutUes18q91q1bF2pccMfw4cPNfMOGDSU8krLLa5X12LFjzdxaqS1J27ZtC9mYSgpXzAAAABxBMQMAAHAExQwAAMARFDMAAABHUMwAAAAcUeZWZf78889mPmnSJDPv27evmf/f//2fmT/++ONBjWfLli0BWc+ePc1js7KyzLxFixZmfvvttwc1FqC0tWvXzsyvvPJKMw9mnzuv1ZFvvPGGmT/66KNmvmfPHjP3+p7gtXdst27dzDwc9+5DXl77OCJ0nnrqqaCO99rjNhzx6gIAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwRJlblell2bJlZr5mzRozP3z4sJm3adPGzMeMGWPm1sovr9WXXr744gszHzduXFDnAUpK27ZtzXzVqlVmXqFCBTP3+/1mvnLlyoDMa1/Nzp07m/m0adPM3Gs12P79+838n//8p5nn5OSYudcKVK89Oj/99FMzR/Hz2te0Ro0aJTyS8ifYfay9vreEI66YAQAAOIJiBgAA4AiKGQAAgCMoZgAAAI6gmAEAADii3KzK9JKZmRnU8YcOHQrq+LFjxwZkL730knms1youwFVNmzY1c6+9ab1WWh04cMDM9+7da+bPPvtsQHbkyBHz2LfeeiuovLjFxcWZ+V133WXmQ4YMKc7h4Hf06dPHzL3+DRE8rxWuDRo0COo8P/74YyiG4wSumAEAADiCYgYAAOAIihkAAIAjKGYAAACOoJgBAAA4otyvygxWcnKymbdr187MrX36evToYR777rvvFnpcQHGKiYkxc2svWMl7NZvXHrTDhw83882bN5t5WVwVd+6555b2EPAb5513XlDHe+1rDG9e30O8Vmt+8803Zu71vSUcccUMAADAERQzAAAAR1DMAAAAHEExAwAAcATFDAAAwBGsygxSVlaWmVt7YkrSp59+GpA9+eST5rFpaWlm7rUybc6cOWbu9/vNHCisCy64wMy9Vl96ufrqq838gw8+CHpMgGs2bdpU2kMoMRUqVDDz3r17m/nQoUPNvFevXkE97n333WfmBw8eDOo8LuOKGQAAgCMoZgAAAI6gmAEAADiCYgYAAOAIfvk/RHbs2GHmI0eODMgWLFhgHjts2LCg8oSEBDNfuHChme/du9fMgfz85S9/MXOfz2fmXr/MX55+yT8iwv65Nycnp4RHgpJSpUqVYj1/mzZtzNxrHnpt/1e3bl0zj46ODsiGDBliHuv1+j527JiZf/zxx2Z+4sQJM4+KsuvJP/7xDzMvS7hiBgAA4AiKGQAAgCMoZgAAAI6gmAEAADiCYgYAAOAIVmUWs6VLlwZk3377rXms18q37t27m/msWbPMvF69emb+wAMPmPmPP/5o5ih/+vbta+Zt27Y1c6/tv15//fVQDSlsea2+9Po727JlSzGOBoXhtcLQ69/w73//u5nfe++9IRlP69atzdxrVeapU6fM/OjRo2b+5ZdfBmTz5883j/XaKtBr5fW+ffvM/IcffjDzuLg4M9+2bZuZlyVcMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwBKsyS8Hnn39u5tddd52ZX3XVVWbutefm+PHjzbxJkyZm3rNnTzNH+eO1EsraQ0+S0tPTzfyll14K2ZhcERMTY+bJyclBnWfNmjVmPmXKlGCHhGI2ceJEM9+1a5eZX3LJJcU5HO3evdvMly1bZuZfffWVmW/cuDFUQyqwcePGmXn16tXN/LvvvivO4TiNK2YAAACOoJgBAAA4gmIGAADgCIoZAACAIyhmAAAAjmBVpkMOHjxo5osWLTLzp556ysyjoux/1k6dOpl5ly5dzPz99983c+CMEydOmPnevXtLeCSh47X6ctq0aWY+adIkM/faA/Cxxx4z8yNHjhRgdHDBww8/XNpDCDteez57WbJkSTGNxH1cMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwBKsyS0Hr1q3NfODAgWbeoUMHM/dafenlyy+/NPMPP/wwqPMAZ7z++uulPYRCa9u2rZl7rbK8/vrrzXz58uVmPmDAgEKNC4C0dOnS0h5CqeGKGQAAgCMoZgAAAI6gmAEAADiCYgYAAOAIihkAAIAjWJUZIuedd56Z33LLLQFZ//79zWNr1qwZkrFkZ2ebudf+hTk5OSF5XIQ/n88XVH7NNdeY+e233x6qIRXZHXfcYeZ//vOfzbxixYpmvnjxYjMfPnx44QYGAAaumAEAADiCYgYAAOAIihkAAIAjKGYAAACOoJgBAAA4glWZHrxWSA4ePNjMrdWXklS/fv1QDSnA5s2bzfyBBx4w83De1xAlw+/3B5V7zZPHH3/czOfPn2/mGRkZZt6xY0czHzZsWEDWpk0b89i6deua+e7du838nXfeMfO5c+eaOYDC81rx3bRpUzPfuHFjcQ7HCVwxAwAAcATFDAAAwBEUMwAAAEdQzAAAABxBMQMAAHBEuVmVWaNGDTNv3ry5mf/tb38z82bNmoVsTL/18ccfm/kjjzxi5suXLzdz9r5ESYmMjDTziRMnmvmAAQPMPDMz08ybNGlSuIH9h/Xr15t5WlqamU+fPr3IjwmgYLxWfEdElN/rRuX3mQMAADiGYgYAAOAIihkAAIAjKGYAAACOoJgBAAA4ImxXZVapUsXM582bZ+Zt27Y184YNG4ZqSCZrRdhjjz1mHuu1R9+xY8dCOibAy4YNG8x806ZNZt6hQ4egzu+1t6bXqmkv1t6aL774onns7bffHtS5AZS+iy++2MyfeeaZkh1IKeCKGQAAgCMoZgAAAI6gmAEAADiCYgYAAOAIihkAAIAjnFmV+Yc//MHMJ02aZOYXXXSRmdepUydkY7IcPXrUzB9//HEznzVrVkCWlZUV0jEBofLDDz+Yef/+/c18/PjxZj5t2rSQjGf27Nlm/j//8z8B2fbt20PymABKjs/nK+0hOIcrZgAAAI6gmAEAADiCYgYAAOAIihkAAIAjKGYAAACOcGZV5rXXXhtUHqwvv/zSzN98800zP3XqlJl77XN58ODBQo0LCAd79+418+Tk5KByAOXTypUrzXzQoEElPBL3ccUMAADAERQzAAAAR1DMAAAAHEExAwAAcATFDAAAwBE+v9/vz++gzMxMVaxYsSTGAxTKoUOHVKFChdIeRlCYV3Ad8woIvfzmFVfMAAAAHEExAwAAcATFDAAAwBEUMwAAAEdQzAAAABxBMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwBMUMAADAERQzAAAAR1DMAAAAHEExAwAAcATFDAAAwBEUMwAAAEcUqJj5/f7iHgdQJOH4Gg3HMaN8CcfXaDiOGeVLfq/RAhWzw4cPh2QwQHEJx9doOI4Z5Us4vkbDccwoX/J7jfr8BfjxIicnR3v27FFSUpJ8Pl/IBgcUld/v1+HDh1W7dm1FRITXO/PMK7iKeQWEXkHnVYGKGQAAAIpfeP0oBAAAUIZRzAAAABxBMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwBMUMAADAERQzAAAAR1DMAAAAHEExAwAAcATFDAAAwBEUMwAAAEdQzAAAABxBMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwBMUMAADAERQzAAAAR1DMAAAAHEExAwAAcESpFrOtW7dqzJgxatSokeLi4hQXF6cmTZpo/Pjx2rx5c2kOrch8Pp+Sk5M9b+/SpYt8Pl++f37vHAVx9OhRJScn6/333w+4LTk5WT6fTwcOHCjSY/zWm2++qeHDh6tVq1Y666yz5PP5Qnp+/D7mVdmcVydPntT06dPVoEEDRUdHq169epoyZYqOHTsW0seBjXlV9uZVZmamHnjgAXXp0kU1a9ZUYmKiWrVqpYcffljHjx8P2eMEK6q0HnjevHm65ZZbdN555+n2229XixYt5PP59NVXX+mFF15Qhw4dtH37djVq1Ki0hlis5s6dq8zMzNyv33rrLd1///1asGCBmjVrlpvXrVu3SI9z9OhRpaSkSDo9uUrC0qVLtXHjRl1wwQWKiYnRP/7xjxJ5XDCvyvK8Gjx4sFasWKHp06erQ4cO2rBhg+6//3598cUXev3110tkDOUV86pszqvdu3frr3/9q4YNG6Y777xTiYmJ+uijj5ScnKxVq1Zp1apVpXJhoVSK2bp16zRx4kRdeeWVevXVVxUdHZ17W7du3XTzzTfrlVdeUVxc3O+e5+jRo4qPjy/u4RaL5s2b5/l627ZtkqSWLVuqffv2nvcLh+f85JNPKiLi9MXYW265hWJWQphXZXdebdy4Ua+99poee+wx3XnnnZKkHj16KCoqSvfee69WrVqlnj17lvIoyybmVdmdVw0aNNDOnTuVkJCQm3Xr1k0JCQmaNGmS1q1bp8suu6zEx1Uqb2XOmjVLkZGRmjdvXp4X+X8aNGiQateunfv1yJEjlZiYqM8++0y9evVSUlKSunfvLkn6+eefNXHiRNWpU0fR0dFq2LChpk6dqhMnTuTef+fOnfL5fHrmmWcCHuu3l2DPXDL94osvNHjwYFWsWFE1atTQ6NGjdejQoTz3zczM1NixY1W1alUlJiaqd+/e+uabb4rwt/NvZ8bx6aefauDAgapcuXLuT2RdunQxf6IYOXKk6tevn/ucq1evLklKSUnJvdw8cuTIPPfZt29fvs8zGGdKGUoW86pgwnFerVu3TpLUp0+fPHnfvn0lSUuWLCnUeZE/5lXBhOO8SkhIyFPKzrjoooskSd9//32hzltUJX7FLDs7W2lpaWrfvr1q1aoV1H1Pnjypfv36afz48brnnnt06tQpHT9+XF27dtWOHTuUkpKi1q1b66OPPtKDDz6oLVu26K233ir0WAcMGKDrr79eY8aM0WeffaYpU6ZIkubPny9J8vv9uuaaa7R+/frctxfWrVunK664otCPaenfv7/++Mc/asKECcrKyirw/WrVqqW3335bvXv31pgxY3TjjTdKUu6L/4z8nqd0etKlpKQoLS2txN66QcExr4IXTvPq5MmTkqSYmJg8+Zmvt27dWuDxo+CYV8ELp3nlZc2aNZKkFi1aBH3fUCjxYnbgwAEdO3ZM9erVC7gtOztbfr8/9+vIyMg87+/++uuvmj59ukaNGpWbzZs3T1u3btXLL7+sQYMGSZJ69uypxMRETZ48uUiX+MeMGaNJkyZJOv22wfbt2zV//nw9/fTT8vl8euedd5SWlqbZs2frtttuy33s6OhoTZ06tVCPaRkxYkTu++7BiImJUbt27SSdfu+/Y8eO5nH5PU/p9FWw3/57wB3Mq+CF07w681bSunXr1KBBg9x87dq1kqSMjIygnwfyx7wKXjjNK8vWrVuVmpqqa6+9Vq1btw76/qHg1HtO7dq101lnnZX757HHHgs4ZsCAAXm+XrNmjRISEjRw4MA8+ZnLn6tXry70ePr165fn69atW+v48eNKT0+XJKWlpUmShgwZkue4G264odCPafntcw61/J6nJE2fPl2nTp1S586di3UsCD3mlS2c5tUVV1yhxo0b5/7nffDgQb399tu69957FRkZya8PlALmlS2c5tVv7dy5U3379tU555yjp556KiTjLYwSn83VqlVTXFycdu3aFXDb888/r02bNnmuMIqPj1eFChXyZBkZGapZs2ZAMz777LMVFRVVpJ8kq1atmufrM28bnFmenpGRoaioqIDjatasWejHtAR7CT1Y+T1PuI95FbxwmlfR0dFauXKlzj33XPXq1UuVK1fWwIEDde+996py5cqqU6dOSMaMvJhXwQunefWfdu3apa5duyoqKkqrV69WlSpVinS+oijxYhYZGalu3bpp8+bN2rt3b57bmjdvrvbt26tVq1bmfa3LklWrVtW+ffvyXFKWpPT0dJ06dUrVqlWTJMXGxkpSnl+wlIr2FkDVqlV16tSpgHP89NNPhT6nxXresbGxAc9FUsg/OwnhgXkVvHCbV40bN9aGDRv0ww8/aOvWrUpPT9egQYN04MABderUqbSHVyYxr4IXbvNKOl3KunTpIr/fr7S0tCJ/7EdRlcr17ylTpig7O1sTJkzQr7/+WqRzde/eXUeOHNGyZcvy5AsXLsy9XZJq1Kih2NjYgF+SXb58eaEfu2vXrpKkxYsX58mff/75Qp+zoOrXr69vvvkmz4s9IyND69evz3McV7/KD+ZV0YXDvKpTp45atWql+Ph4PfLII0pISNCYMWNKfBzlBfOq6FyeV7t371aXLl2UnZ2tNWvWmL9PWNJK5XPMLr30Us2ZM0e33nqrLrzwQo0bN04tWrRQRESE9u7dm7v0+7eXgS3Dhw/XnDlzNGLECO3cuVOtWrXS2rVrNWvWLPXp00c9evSQdLrFDx06VPPnz1ejRo3Upk0bffLJJ0V6Ufbq1UudOnXSn/70J2VlZal9+/Zat26dFi1aVOhzFtSwYcM0b948DR06VGPHjlVGRoZSU1MD/s6SkpJUr149LV++XN27d1eVKlVUrVq13CXKBTVz5kzNnDlTq1evzvd9+127dmnTpk2SpB07dkiSXn31VUmnJ+jvfe4NCo95VXQuz6vU1FTVrFlT5557rvbt26eXX35Zy5Yt06JFi3grsxgxr4rO1XmVnp6url27au/evXr66aeVnp6e53fV6tatWzpXz/ylaMuWLf5Ro0b5GzRo4I+JifHHxsb6Gzdu7B8+fLh/9erVeY4dMWKEPyEhwTxPRkaGf8KECf5atWr5o6Ki/PXq1fNPmTLFf/z48TzHHTp0yH/jjTf6a9So4U9ISPBfddVV/p07d/ol+WfMmJF73IwZM/yS/Pv3789z/wULFvgl+f/1r3/lZgcPHvSPHj3aX6lSJX98fLy/Z8+e/m3btgWcMz9nzr1p06Z8x3HGs88+6z///PP9sbGx/ubNm/tfeukl/4gRI/z16tXLc9x7773nv+CCC/wxMTF+Sf4RI0YE/TzPHJuWllbg52L9OfPYKD7Mq8Bzl4V5lZKS4m/UqJE/JibGX6lSJX/v3r39H374YYH+HlB0zKvAc4f7vEpLS/P8vyrYv5NQ8vn9v3mzGwAAAKWCNdYAAACOoJgBAAA4gmIGAADgCIoZAACAIyhmAAAAjqCYAQAAOKJAHzCbk5OjPXv2KCkpqVC7tQPFxe/36/Dhw6pdu3bYbeTMvIKrmFdA6BV0XhWomO3Zs0fnnHNOyAYHhNr3339f6vubBYt5Bdcxr4DQy29eFehHoaSkpJANCCgO4fgaDccxo3wJx9doOI4Z5Ut+r9ECFTMuB8N14fgaDccxo3wJx9doOI4Z5Ut+r9Hw+uUBAACAMoxiBgAA4AiKGQAAgCMoZgAAAI6gmAEAADiCYgYAAOAIihkAAIAjKGYAAACOoJgBAAA4gmIGAADgCIoZAACAIyhmAAAAjqCYAQAAOIJiBgAA4AiKGQAAgCMoZgAAAI6gmAEAADiCYgYAAOAIihkAAIAjKGYAAACOoJgBAAA4gmIGAADgCIoZAACAIyhmAAAAjqCYAQAAOIJiBgAA4AiKGQAAgCOiSnsACL1p06aZeUpKiplHRNj9vEuXLmb+wQcfFGpcAICyISkpycwTExPN/MorrzTz6tWrm/lf/vIXMz9x4kQBRhfeuGIGAADgCIoZAACAIyhmAAAAjqCYAQAAOIJiBgAA4AhWZYaxkSNHmvnkyZPNPCcnJ6jz+/3+YIcEAAhT9evXD8i8/j+5+OKLzbxly5YhGUutWrXM/LbbbgvJ+V3GFTMAAABHUMwAAAAcQTEDAABwBMUMAADAERQzAAAAR7AqM4zVq1fPzGNjY0t4JEDx+MMf/mDmQ4cODcg6d+5sHtuiRYugHvPuu+828z179pj5ZZddZubPPfecmX/88cdBjQcorGbNmpn5f/3Xf5n5kCFDArK4uDjzWJ/PZ+bff/+9mR8+fNjMzz//fDO/7rrrzHzu3Llmvm3bNjMPR1wxAwAAcATFDAAAwBEUMwAAAEdQzAAAABxBMQMAAHAEqzLDQI8ePcz81ltvDeo8XqtW+vbta+b79u0L6vxAYV1//fVmPnv2bDOvVq1aQOa1Suz999838+rVq5v5I488YuZevB7X6/x//OMfgzo/cEbFihXN/OGHHzZzr3mVlJRU5LF8++23Zn755Zeb+VlnnWXmXv8vWXP89/KyhCtmAAAAjqCYAQAAOIJiBgAA4AiKGQAAgCMoZgAAAI5gVaZDvPbcW7BggZl7rdDx4rXabNeuXUGdB8hPVJT9raV9+/Zm/uSTT5p5fHy8mX/44YcB2X333Wceu3btWjOPiYkx85dfftnMe/XqZeZeNm/eHNTxQH6uvfZaM7/xxhuL7TF37Nhh5j179jRzr70yGzduHLIxlXVcMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwBKsyHTJixAgzr127dlDn8dobcOHChcEOCSiUoUOHmvlTTz0V1HlWrVpl5tYegJmZmUGd22sfwWBXX/7www9m/uyzzwZ1HiA/gwYNCsl5du7caeabNm0KyCZPnmwe67X60sv5558f1PHlGVfMAAAAHEExAwAAcATFDAAAwBEUMwAAAEdQzAAAABzBqsxSUK1aNTMfPXq0mefk5Jj5wYMHzfz+++8v1LiAYHntT3nvvfeaud/vN/O5c+ea+bRp08w82BWYlqlTpxb5HJJ02223mfn+/ftDcn7gjLFjx5r5uHHjzPzdd9818+3bt5t5enp64QZWADVq1Ci2c5c1XDEDAABwBMUMAADAERQzAAAAR1DMAAAAHEExAwAAcASrMotZ/fr1A7IlS5aE5NxPPPGEmaelpYXk/MAZ06dPN3Ov1ZcnT54083feecfMvfbjO3bsWAFGd1psbKyZe+19ee6555q5z+czc6/VzsuXLy/A6ICi27Nnj5knJyeX7EAK4eKLLy7tIYQNrpgBAAA4gmIGAADgCIoZAACAIyhmAAAAjqCYAQAAOIJVmcWsd+/eAVnr1q2DOsfq1avNfPbs2YUaE+ClUqVKZj5x4kQz99r70mv15TXXXFOYYQVo3LhxQLZ48WLz2Hbt2gV17ldffdXMU1NTgzoPEG6sfV8TEhJCcu5WrVoFdfz69evNfMOGDaEYjtO4YgYAAOAIihkAAIAjKGYAAACOoJgBAAA4gl/+DxGvX2p+6KGHCnyOtWvXmvmIESPM/NChQwU+N1AQ0dHRZl6tWrWgzmP9ErEknX322WY+atQoM+/Xr5+Zt2zZMiBLTEw0j/VaoOCVP/fcc2aelZVl5kBpi4+PN/PmzZub+YwZM8y8T58+BX7MiAj7uk5OTk6BzyF5bzPl9T0hOzs7qPOHI66YAQAAOIJiBgAA4AiKGQAAgCMoZgAAAI6gmAEAADiCVZlBql+/vpkvWbKkyOf+7rvvzHzfvn1FPjdQECdPnjTz/fv3m3n16tXN/F//+peZe62EDJa1kiszM9M8tlatWmZ+4MABM3/jjTcKPzAgBM466ywzv+CCC8zc6/8fr9f+sWPHzNyaV15bIFnbDUreK0S9REXZNaR///5m7rUVodf3rnDEFTMAAABHUMwAAAAcQTEDAABwBMUMAADAERQzAAAAR7AqM0iTJ08282D3B7MEs68mUBwOHjxo5l57wb755ptmXqVKFTPfsWOHmS9fvtzMn3nmGTP/+eefA7IXX3zRPNZrZZrX8UBJ8dqb1mvF42uvvRbU+VNSUsx8zZo1Zr5u3bqAzGsue53D2sf293it7H7wwQfNfPfu3Wa+bNkyMz9x4kRQ43EBV8wAAAAcQTEDAABwBMUMAADAERQzAAAAR1DMAAAAHMGqTA9t27Y18169ehX53F4r0L7++usinxsoDh9//LGZe62oKm6dOnUKyDp37mwe67Vi2mtvWiDUvPa+9Fo1OWnSpKDOv3LlSjN/4oknzNxr9bU1n1esWGEe26pVKzP32rMyNTXVzL1WcV599dVmvnjxYjN/7733zPzhhx82819++cXMvWzZsiWo44uCK2YAAACOoJgBAAA4gmIGAADgCIoZAACAIyhmAAAAjmBVpod3333XzCtXrhzUeTZu3BiQjRw5sjBDAvD/xcXFBWReqy/9fr+Zs1cmQi0yMtLM77vvPjO/++67zTwrK8vM77nnHjP3ei17rb5s3769mf/tb38LyC644ALz2G+//dbMb7rpJjNPS0sz8woVKpj5JZdcYuZDhgwx8379+pn5qlWrzNzL999/b+YNGjQI6jxFwRUzAAAAR1DMAAAAHEExAwAAcATFDAAAwBEUMwAAAEewKtND1apVzdxr5ZeXuXPnBmRHjhwp1JgAnPbOO++U9hCAAOPGjTNzr9WXR48eNfPx48ebudenBXTs2NHMR40aZeZXXHGFmVurnWfOnGkeu2DBAjP3WtXoJTMz08zffvvtoPLBgweb+Q033BDUeO64446gji8OXDEDAABwBMUMAADAERQzAAAAR1DMAAAAHEExAwAAcITP77WR3H/IzMxUxYoVS2I8Jc5rZYnXfpbBrsps2LBhQLZr166gzoH8HTp0yHPPNVeV5XlV3C6//PKAbMWKFeaxXt/iatWqZeb79+8v/MDKGOZVcPbu3Wvm1atXN/MTJ06Y+bZt28w8ISHBzBs3blyA0eUvOTk5IHvwwQfNY7Ozs0PymOVRfvOKK2YAAACOoJgBAAA4gmIGAADgCIoZAACAIyhmAAAAjig3e2W2bdvWzHv06GHmXqsvT548aeZz5swx83379uU/OABBsVY7A6Xtp59+MnOvVZkxMTFm3qZNm6Ae12tF8ocffmjmy5YtM/OdO3cGZKy+LHlcMQMAAHAExQwAAMARFDMAAABHUMwAAAAcQTEDAABwRLlZlVmpUiUzr1mzZlDn+fHHH8387rvvDnZIAArpo48+CsgiIuyfM4Pd3xYorE6dOpn5NddcY+YXXnihmaenp5v5/PnzzfyXX34xc69PEYDbuGIGAADgCIoZAACAIyhmAAAAjqCYAQAAOIJiBgAA4IhysyoTQNnx+eefB2TffvuteazXvpqNGjUy8/379xd+YCjXDh8+bOaLFi0KKkf5xhUzAAAAR1DMAAAAHEExAwAAcATFDAAAwBEUMwAAAEeUm1WZ27ZtM/P169eb+WWXXVacwwEQYrNmzTLzp556yswfeOABM7/11lvN/MsvvyzcwAAgCFwxAwAAcATFDAAAwBEUMwAAAEdQzAAAABxBMQMAAHCEz+/3+/M7KDMzUxUrViyJ8QCFcujQIVWoUKG0hxEU5lVoef37v/zyy2beo0cPM3/ttdfMfNSoUWaelZVVgNGFJ+YVEHr5zSuumAEAADiCYgYAAOAIihkAAIAjKGYAAACOoJgBAAA4otzslQmgbMvMzDTz6667zsy99sq86aabzDw5OdnM2UMTQChxxQwAAMARFDMAAABHUMwAAAAcQTEDAABwBMUMAADAEeyViTKBPf2A0GNeAaHHXpkAAABhgmIGAADgCIoZAACAIyhmAAAAjihQMSvA+gCgVIXjazQcx4zyJRxfo+E4ZpQv+b1GC1TMDh8+HJLBAMUlHF+j4ThmlC/h+BoNxzGjfMnvNVqgj8vIycnRnj17lJSUJJ/PF7LBAUXl9/t1+PBh1a5dWxER4fXOPPMKrmJeAaFX0HlVoGIGAACA4hdePwoBAACUYRQzAAAAR1DMAAAAHEExAwAAcATFDAAAwBEUMwAAAEdQzAAAABzx/wAy/11JEWTjYQAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show some images of mnist\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout() #自动检查子图的坐标轴，标题和刻度标签\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:35:59.477934600Z",
     "start_time": "2024-05-17T09:35:58.299125600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model: you may select your favorite model and implement it in the following class."
   ],
   "metadata": {
    "id": "ovgrtfvNa-VF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#define the network: the convolution and pool in the Convolution neural network should satisfy the computing rules, such the dimension of the H_out. The method of calculating is described in the doc of torch.       \n",
    "class Model(nn.Module):\n",
    "  # Your code here\n",
    "  def __init__(self):\n",
    "      super(Model,self).__init__()\n",
    "      #CIFAR 10 model\n",
    "      self.model = nn.Sequential(\n",
    "          nn.Conv2d(1,32,5,1,2),\n",
    "          nn.MaxPool2d(2),\n",
    "          nn.Conv2d(32,64,5,1,2),\n",
    "          nn.MaxPool2d(2),\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(64*7*7,64),\n",
    "          nn.Linear(64,10)\n",
    "      )\n",
    "  def forward(self,x):\n",
    "      #img-->feature-->score_classes\n",
    "      x = self.model(x)\n",
    "      return x\n",
    "\n",
    "  # End code here"
   ],
   "metadata": {
    "id": "I_Bo3r3FbIWO",
    "ExecuteTime": {
     "end_time": "2024-05-17T09:35:59.516833600Z",
     "start_time": "2024-05-17T09:35:59.481986100Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1233,  0.0553,  0.0785, -0.0748,  0.0280, -0.0241, -0.0611, -0.0688,\n",
      "          0.1152,  0.0015]], grad_fn=<AddmmBackward0>) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# give an example\n",
    "model = Model()\n",
    "input = torch.ones((1,1,28,28))\n",
    "output = model(input)     \n",
    "print(output,output.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:35:59.536945900Z",
     "start_time": "2024-05-17T09:35:59.494120100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation: you need to implement a pipeline to train your neural network."
   ],
   "metadata": {
    "id": "F2IadIrIbU8V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# train and test on CPU\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "model = Model()\n",
    "# 1. put the model on GPU\n",
    "model = model.to(device) # method 1\n",
    "# if torch.cuda.is_available(): # method 2\n",
    "#     model = model.cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# 2. put the loss function on GPU\n",
    "loss_func =loss_func.to(device)\n",
    "learn_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learn_rate)\n",
    "\n",
    "epochs = 100\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "#add the tensorboard\n",
    "writer = SummaryWriter(\"../logs_train\")\n",
    "for epoch in range(epochs):\n",
    "  # Your code here\n",
    "  print(\"--------第{}轮训练开始-----\".format(epoch+1))\n",
    "  #start the train\n",
    "  model.train() # the \n",
    "  for data in train_dataloader:\n",
    "      imgs, targets = data\n",
    "      # imgs = imgs.convert(\"RGB\")\n",
    "      #3. put the data and target on GPU\n",
    "      imgs = imgs.to(device)\n",
    "      targets = targets.to(device)\n",
    "      \n",
    "      #calculating the output by the model class and calculating the loss by crossEntropy function \n",
    "      outputs = model(imgs)\n",
    "      loss = loss_func(outputs,targets)\n",
    "      \n",
    "      # to optimize\n",
    "      optimizer.zero_grad()       \n",
    "      loss.backward() \n",
    "      optimizer.step() #update the grad\n",
    "      \n",
    "      total_train_step = total_train_step + 1\n",
    "      if total_train_step %100 == 0: \n",
    "        print(\"训练次数：{}，loss:{}\".format(total_train_step,loss.item()))\n",
    "        writer.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "  #start the test\n",
    "  model.eval()\n",
    "  total_test_loss = 0\n",
    "  total_accuracy = 0\n",
    "  with torch.no_grad():\n",
    "      for data in test_dataloader:\n",
    "          imgs, targets = data\n",
    "          imgs = imgs.to(device)\n",
    "          targets = targets.to(device)\n",
    "          outputs = model(imgs)\n",
    "          loss = loss_func(outputs,targets)\n",
    "          total_test_loss = total_test_loss + loss.item()\n",
    "          accuracy = (outputs.argmax(1) == targets).sum()\n",
    "          total_accuracy = total_accuracy + accuracy\n",
    "          \n",
    "  print(\"整体测试集上的Loss：{}\".format(total_test_loss))\n",
    "  print(\"整体的正确率:{}\".format(total_accuracy/len(test_dataloader)))\n",
    "  total_test_step = total_test_step + 1\n",
    "  writer.add_scalar(\"test_loss\",loss.item(),total_test_step)\n",
    "  writer.add_scalar(\"acc\",total_accuracy/len(test_dataloader),total_test_step)\n",
    "  torch.save(model,\"./model_save/model_mnist_{}.pth\".format(epoch))\n",
    "  print(\"模型已保存\")\n",
    "writer.close()\n",
    "      \n",
    "  # End code here"
   ],
   "metadata": {
    "id": "0RKqr0UebjDf",
    "ExecuteTime": {
     "end_time": "2024-05-17T12:47:56.037453800Z",
     "start_time": "2024-05-17T12:47:40.018303200Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------第1轮训练开始-----\n",
      "训练次数：100，loss:0.2148481160402298\n",
      "训练次数：200，loss:0.15477855503559113\n",
      "训练次数：300，loss:0.16831853985786438\n",
      "训练次数：400，loss:0.32520565390586853\n",
      "训练次数：500，loss:0.30237361788749695\n",
      "训练次数：600，loss:0.13863657414913177\n",
      "训练次数：700，loss:0.26564523577690125\n",
      "训练次数：800，loss:0.3787654638290405\n",
      "训练次数：900，loss:1.46573805809021\n",
      "整体测试集上的Loss：11122.97951889038\n",
      "整体的正确率:43.50955581665039\n",
      "模型已保存\n",
      "--------第2轮训练开始-----\n",
      "训练次数：1000，loss:50.99159622192383\n",
      "训练次数：1100，loss:0.36925506591796875\n",
      "训练次数：1200，loss:7.7446746826171875\n",
      "训练次数：1300，loss:6.737908363342285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m#start the train\u001B[39;00m\n\u001B[0;32m     24\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain() \u001B[38;5;66;03m# the \u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[0;32m     26\u001B[0m     imgs, targets \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m# imgs = imgs.convert(\"RGB\")\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m#3. put the data and target on GPU\u001B[39;00m\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\HMCAN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\HMCAN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\HMCAN\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\HMCAN\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\HMCAN\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    142\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 145\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    148\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\HMCAN\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\HMCAN\\lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    169\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m255\u001B[39m \u001B[38;5;241m*\u001B[39m img\n\u001B[1;32m--> 170\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_image_num_channels\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# put it from HWC to CHW format\u001B[39;00m\n\u001B[0;32m    172\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#an example of tensorboard \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "img_path = '../results/line.png'\n",
    "img = Image.open(img_path)\n",
    "tran = transforms.ToTensor()\n",
    "# img.show()\n",
    "writer = SummaryWriter('logs')\n",
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_img = tensor_trans(img)\n",
    "writer.add_image(\"tensor_img\", tensor_img)\n",
    "writer.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T10:20:42.469380500Z",
     "start_time": "2024-05-17T10:20:42.375294600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类中初始化函数和构造函数的定义与调用\n",
    "class Box:\n",
    "    def setDimension(self,width,height,depth):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.depth = depth\n",
    "    def getVolume(self):\n",
    "        return self.height*self.width*self.depth\n",
    "b = Box()\n",
    "b.setDimension(1,2,3)\n",
    "b.getVolume()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T10:20:45.027232600Z",
     "start_time": "2024-05-17T10:20:44.875304100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
