{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this work, you are required to build a GNN training pipline. Then you can truly use the Graph Neural Network."
   ],
   "metadata": {
    "id": "_AfnlcwOWS5l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install  dgl -f https://data.dgl.ai/wheels/repo.html\n",
    "# !pip install torch_geometric\n",
    "# !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cpu.html"
   ],
   "metadata": {
    "id": "7H0-sw8zbcrn",
    "ExecuteTime": {
     "end_time": "2024-07-04T05:25:45.428337300Z",
     "start_time": "2024-07-04T05:25:45.400253200Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need to download the dataset and load data."
   ],
   "metadata": {
    "id": "Ex3YFUE3Whks"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jLg7RaF6WRTt",
    "ExecuteTime": {
     "end_time": "2024-07-04T05:25:54.890118700Z",
     "start_time": "2024-07-04T05:25:45.416256Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(\"../\", \"Cora\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "x = data.x\n",
    "# edge_index = data.edge_index\n",
    "# edge_weight = data.edge_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, you need to implement a GNN model. You may copy the GCNConv from your work two weeks ago, and build the model with the convolution layers."
   ],
   "metadata": {
    "id": "0MA85AI9b8yX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "# class PyG_GCNConv(MessagePassing):\n",
    "#   # Your code here\n",
    "#   def __init__(self, in):\n",
    "#       \n",
    "#   # End code here\n",
    "# \n",
    "\n",
    "class PyG_GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PyG_GCNConv, self).__init__(aggr='add')  # 聚合函数为加法\n",
    "        self.input_channel = in_channels\n",
    "        self.output_channel = out_channels\n",
    "        self.W = torch.nn.Parameter(torch.ones(in_channels, out_channels))\n",
    "        self.bias = torch.nn.Parameter(torch.ones(out_channels))\n",
    "        # torch.nn.init.xavier_uniform_(self.W)  # 使用Xavier初始化\n",
    "        # torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index,edge_weight):\n",
    "        # 可选：为邻接矩阵添加自环\n",
    "        # edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        \n",
    "        # print(\"edge_index:\", edge_index.shape)\n",
    "        # print(\"x before linear:\", x.shape)\n",
    "        \n",
    "        # 线性变换\n",
    "        x = torch.matmul(x, self.W)\n",
    "        \n",
    "        # print(\"x after linear:\", x.shape)\n",
    "        \n",
    "        # 开始消息传递\n",
    "        out = self.propagate(edge_index, x=x,edge_weight=edge_weight)\n",
    "        \n",
    "        # 添加偏置\n",
    "        out += self.bias\n",
    "        return out\n",
    "\n",
    "    # 重写message函数\n",
    "    def message(self, x_j, edge_weight,edge_index):\n",
    "        # print(\"x_j:\", x_j.shape)\n",
    "        if edge_weight is None:\n",
    "            return x_j\n",
    "        else:\n",
    "            # x = x_j[edge_index[0,:]]\n",
    "            # print(\"edge_weight:\", edge_weight.shape)\n",
    "            return edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    # 如有需要，可以重写update函数（可选）\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "class PyG_GCN(nn.Module):\n",
    "  # Your code here\n",
    "  def __init__(self,in_channels, out_channels, hidden_dim):\n",
    "      super(PyG_GCN,self).__init__()\n",
    "      self.conv1 = PyG_GCNConv(in_channels,hidden_dim)\n",
    "      self.conv2 = PyG_GCNConv(hidden_dim, out_channels)\n",
    "  def forward(self,data):\n",
    "      #传入进数据\n",
    "      x, edge_index = data.x, data.edge_index\n",
    "      #经过每层网络\n",
    "      x = self.conv1(x,data.edge_index,data.edge_weight)\n",
    "      x = F.relu(x)\n",
    "      x = F.dropout(x, training = self.training)\n",
    "      x = self.conv2(x, data.edge_index,data.edge_weight)\n",
    "      return F.log_softmax(x,dim=1)\n",
    "  # End code here"
   ],
   "metadata": {
    "id": "B4La5uI1cQQA",
    "ExecuteTime": {
     "end_time": "2024-07-04T05:25:54.916504700Z",
     "start_time": "2024-07-04T05:25:54.899452500Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building the training and evaluation part, this is similar to the work in week4. Our downstream task is just node classification."
   ],
   "metadata": {
    "id": "Wz1whBYkcdtx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch_geometric.logging import log\n",
    "import torch\n",
    "# Build your training pipeline\n",
    "hidden_dim = 16\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "model = PyG_GCN(dataset.num_features, hidden_dim, dataset.num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "def train():\n",
    "  # Your code here\n",
    "      #梯度是累积的（即每次调用 backward() 时，计算出的梯度会被加到已有的梯度上）。如果不清零梯度，那么在每次更新参数时，梯度都会包含当前批次以及所有之前批次的梯度，这样会导致错误的参数更新。optimizer.zero_grad()用于在每次进行反向传播和更新模型参数之前，将所有模型参数的梯度清零。这样可以避免累积梯度，从而确保每次迭代时梯度计算都是基于当前的批次数据。\n",
    "      optimizer.zero_grad()\n",
    "      out = model(data)\n",
    "      loss = F.nll_loss(out[data.train_mask],data.y[data.train_mask])\n",
    "      #反向传播,计算梯度\n",
    "      loss.backward()\n",
    "      #用于根据上一步计算的梯度更新模型的参数，以最小化损失函数\n",
    "      optimizer.step()\n",
    "      return loss\n",
    "  # End code here\n",
    "# 用于在不需要计算梯度时，临时禁用梯度计算。提高计算效率\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "  model.eval()\n",
    "  pred = model(data).argmax(dim=-1)\n",
    "\n",
    "  accs = []\n",
    "  for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "      accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "  return accs\n",
    "best_val_acc = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "  loss = train()\n",
    "  train_acc, val_acc, tmp_test_acc = test()\n",
    "  \n",
    "  if val_acc > best_val_acc:\n",
    "      best_val_acc = val_acc\n",
    "      test_acc = tmp_test_acc\n",
    "  log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)"
   ],
   "metadata": {
    "id": "aMlTRvwGcmFw",
    "ExecuteTime": {
     "end_time": "2024-07-04T05:26:00.877226400Z",
     "start_time": "2024-07-04T05:25:54.912627200Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.772589683532715, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
      "Epoch: 002, Loss: 2.4910247325897217, Train: 0.1429, Val: 0.0720, Test: 0.1300\n",
      "Epoch: 003, Loss: 2.366645097732544, Train: 0.1429, Val: 0.0720, Test: 0.1300\n",
      "Epoch: 004, Loss: 2.2941508293151855, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 005, Loss: 2.2474563121795654, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 006, Loss: 2.2103235721588135, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 007, Loss: 2.176870107650757, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 008, Loss: 2.1495609283447266, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 009, Loss: 2.130228281021118, Train: 0.1429, Val: 0.1580, Test: 0.1430\n",
      "Epoch: 010, Loss: 2.1143524646759033, Train: 0.1357, Val: 0.1520, Test: 0.1430\n",
      "Epoch: 011, Loss: 2.100473642349243, Train: 0.1429, Val: 0.1540, Test: 0.1430\n",
      "Epoch: 012, Loss: 2.089570999145508, Train: 0.1429, Val: 0.1560, Test: 0.1430\n",
      "Epoch: 013, Loss: 2.0795352458953857, Train: 0.1429, Val: 0.1560, Test: 0.1430\n",
      "Epoch: 014, Loss: 2.069622755050659, Train: 0.1429, Val: 0.1560, Test: 0.1430\n",
      "Epoch: 015, Loss: 2.060420274734497, Train: 0.1500, Val: 0.1740, Test: 0.1590\n",
      "Epoch: 016, Loss: 2.05326771736145, Train: 0.1500, Val: 0.1700, Test: 0.1590\n",
      "Epoch: 017, Loss: 2.048280954360962, Train: 0.1500, Val: 0.1740, Test: 0.1590\n",
      "Epoch: 018, Loss: 2.0432446002960205, Train: 0.1429, Val: 0.1600, Test: 0.1590\n",
      "Epoch: 019, Loss: 2.037518262863159, Train: 0.1429, Val: 0.1640, Test: 0.1590\n",
      "Epoch: 020, Loss: 2.032320737838745, Train: 0.1429, Val: 0.1700, Test: 0.1590\n",
      "Epoch: 021, Loss: 2.0279438495635986, Train: 0.1500, Val: 0.1520, Test: 0.1590\n",
      "Epoch: 022, Loss: 2.02396559715271, Train: 0.1571, Val: 0.1560, Test: 0.1590\n",
      "Epoch: 023, Loss: 2.0202133655548096, Train: 0.1500, Val: 0.1580, Test: 0.1590\n",
      "Epoch: 024, Loss: 2.0170392990112305, Train: 0.1429, Val: 0.1680, Test: 0.1590\n",
      "Epoch: 025, Loss: 2.0144453048706055, Train: 0.1429, Val: 0.1720, Test: 0.1590\n",
      "Epoch: 026, Loss: 2.011584997177124, Train: 0.1500, Val: 0.1680, Test: 0.1590\n",
      "Epoch: 027, Loss: 2.00836181640625, Train: 0.1500, Val: 0.1680, Test: 0.1590\n",
      "Epoch: 028, Loss: 2.0056095123291016, Train: 0.1500, Val: 0.1800, Test: 0.1760\n",
      "Epoch: 029, Loss: 2.003540277481079, Train: 0.1500, Val: 0.1560, Test: 0.1760\n",
      "Epoch: 030, Loss: 2.0015716552734375, Train: 0.1500, Val: 0.1600, Test: 0.1760\n",
      "Epoch: 031, Loss: 1.9994349479675293, Train: 0.1500, Val: 0.1540, Test: 0.1760\n",
      "Epoch: 032, Loss: 1.9974160194396973, Train: 0.1571, Val: 0.1720, Test: 0.1760\n",
      "Epoch: 033, Loss: 1.9955509901046753, Train: 0.1571, Val: 0.1900, Test: 0.1920\n",
      "Epoch: 034, Loss: 1.9936699867248535, Train: 0.1571, Val: 0.1900, Test: 0.1920\n",
      "Epoch: 035, Loss: 1.991856575012207, Train: 0.1571, Val: 0.1700, Test: 0.1920\n",
      "Epoch: 036, Loss: 1.9903359413146973, Train: 0.1500, Val: 0.1760, Test: 0.1920\n",
      "Epoch: 037, Loss: 1.9889875650405884, Train: 0.1500, Val: 0.1600, Test: 0.1920\n",
      "Epoch: 038, Loss: 1.9875209331512451, Train: 0.1500, Val: 0.1480, Test: 0.1920\n",
      "Epoch: 039, Loss: 1.9859434366226196, Train: 0.1571, Val: 0.1640, Test: 0.1920\n",
      "Epoch: 040, Loss: 1.9845457077026367, Train: 0.1429, Val: 0.1820, Test: 0.1920\n",
      "Epoch: 041, Loss: 1.983302354812622, Train: 0.1429, Val: 0.1960, Test: 0.1990\n",
      "Epoch: 042, Loss: 1.9820491075515747, Train: 0.1429, Val: 0.1960, Test: 0.1990\n",
      "Epoch: 043, Loss: 1.9808449745178223, Train: 0.1571, Val: 0.1860, Test: 0.1990\n",
      "Epoch: 044, Loss: 1.9797202348709106, Train: 0.1500, Val: 0.1580, Test: 0.1990\n",
      "Epoch: 045, Loss: 1.9785469770431519, Train: 0.1357, Val: 0.1580, Test: 0.1990\n",
      "Epoch: 046, Loss: 1.9773707389831543, Train: 0.1357, Val: 0.1580, Test: 0.1990\n",
      "Epoch: 047, Loss: 1.9762872457504272, Train: 0.1500, Val: 0.1980, Test: 0.1850\n",
      "Epoch: 048, Loss: 1.9753080606460571, Train: 0.1429, Val: 0.2060, Test: 0.2030\n",
      "Epoch: 049, Loss: 1.974310278892517, Train: 0.1429, Val: 0.2100, Test: 0.1990\n",
      "Epoch: 050, Loss: 1.9732944965362549, Train: 0.1500, Val: 0.1980, Test: 0.1990\n",
      "Epoch: 051, Loss: 1.97231924533844, Train: 0.1357, Val: 0.1600, Test: 0.1990\n",
      "Epoch: 052, Loss: 1.9713762998580933, Train: 0.1357, Val: 0.1600, Test: 0.1990\n",
      "Epoch: 053, Loss: 1.970436692237854, Train: 0.1357, Val: 0.1740, Test: 0.1990\n",
      "Epoch: 054, Loss: 1.969538688659668, Train: 0.1357, Val: 0.2120, Test: 0.1830\n",
      "Epoch: 055, Loss: 1.9687011241912842, Train: 0.1429, Val: 0.2220, Test: 0.1920\n",
      "Epoch: 056, Loss: 1.9678107500076294, Train: 0.1429, Val: 0.2220, Test: 0.1920\n",
      "Epoch: 057, Loss: 1.966941237449646, Train: 0.1286, Val: 0.2060, Test: 0.1920\n",
      "Epoch: 058, Loss: 1.9661043882369995, Train: 0.1286, Val: 0.1620, Test: 0.1920\n",
      "Epoch: 059, Loss: 1.9652901887893677, Train: 0.1429, Val: 0.1640, Test: 0.1920\n",
      "Epoch: 060, Loss: 1.9644843339920044, Train: 0.1500, Val: 0.1680, Test: 0.1920\n",
      "Epoch: 061, Loss: 1.9636982679367065, Train: 0.1357, Val: 0.2020, Test: 0.1920\n",
      "Epoch: 062, Loss: 1.9629242420196533, Train: 0.1357, Val: 0.2160, Test: 0.1920\n",
      "Epoch: 063, Loss: 1.9621309041976929, Train: 0.1286, Val: 0.2100, Test: 0.1920\n",
      "Epoch: 064, Loss: 1.9613794088363647, Train: 0.1286, Val: 0.1940, Test: 0.1920\n",
      "Epoch: 065, Loss: 1.9606342315673828, Train: 0.1357, Val: 0.1560, Test: 0.1920\n",
      "Epoch: 066, Loss: 1.9599018096923828, Train: 0.1357, Val: 0.1560, Test: 0.1920\n",
      "Epoch: 067, Loss: 1.9591596126556396, Train: 0.1429, Val: 0.1600, Test: 0.1920\n",
      "Epoch: 068, Loss: 1.9584379196166992, Train: 0.1286, Val: 0.1940, Test: 0.1920\n",
      "Epoch: 069, Loss: 1.9577176570892334, Train: 0.1214, Val: 0.1960, Test: 0.1920\n",
      "Epoch: 070, Loss: 1.9570204019546509, Train: 0.1214, Val: 0.1980, Test: 0.1920\n",
      "Epoch: 071, Loss: 1.9563299417495728, Train: 0.1286, Val: 0.1580, Test: 0.1920\n",
      "Epoch: 072, Loss: 1.955635666847229, Train: 0.1286, Val: 0.1620, Test: 0.1920\n",
      "Epoch: 073, Loss: 1.954947590827942, Train: 0.1286, Val: 0.1600, Test: 0.1920\n",
      "Epoch: 074, Loss: 1.9542609453201294, Train: 0.1357, Val: 0.1600, Test: 0.1920\n",
      "Epoch: 075, Loss: 1.9535945653915405, Train: 0.1214, Val: 0.1920, Test: 0.1920\n",
      "Epoch: 076, Loss: 1.9529242515563965, Train: 0.1214, Val: 0.1900, Test: 0.1920\n",
      "Epoch: 077, Loss: 1.9522615671157837, Train: 0.1286, Val: 0.1640, Test: 0.1920\n",
      "Epoch: 078, Loss: 1.9515997171401978, Train: 0.1286, Val: 0.1660, Test: 0.1920\n",
      "Epoch: 079, Loss: 1.9509423971176147, Train: 0.1286, Val: 0.1660, Test: 0.1920\n",
      "Epoch: 080, Loss: 1.9502995014190674, Train: 0.1286, Val: 0.1680, Test: 0.1920\n",
      "Epoch: 081, Loss: 1.949646234512329, Train: 0.1286, Val: 0.1700, Test: 0.1920\n",
      "Epoch: 082, Loss: 1.94900643825531, Train: 0.1357, Val: 0.1680, Test: 0.1920\n",
      "Epoch: 083, Loss: 1.9483790397644043, Train: 0.1286, Val: 0.1720, Test: 0.1920\n",
      "Epoch: 084, Loss: 1.9477355480194092, Train: 0.1286, Val: 0.1640, Test: 0.1920\n",
      "Epoch: 085, Loss: 1.9471027851104736, Train: 0.1286, Val: 0.1640, Test: 0.1920\n",
      "Epoch: 086, Loss: 1.9464608430862427, Train: 0.1286, Val: 0.1640, Test: 0.1920\n",
      "Epoch: 087, Loss: 1.9458328485488892, Train: 0.1286, Val: 0.1700, Test: 0.1920\n",
      "Epoch: 088, Loss: 1.9452017545700073, Train: 0.1286, Val: 0.1720, Test: 0.1920\n",
      "Epoch: 089, Loss: 1.944577693939209, Train: 0.1286, Val: 0.1720, Test: 0.1920\n",
      "Epoch: 090, Loss: 1.9439539909362793, Train: 0.1286, Val: 0.1660, Test: 0.1920\n",
      "Epoch: 091, Loss: 1.9433249235153198, Train: 0.1571, Val: 0.1720, Test: 0.1920\n",
      "Epoch: 092, Loss: 1.9427000284194946, Train: 0.1571, Val: 0.1680, Test: 0.1920\n",
      "Epoch: 093, Loss: 1.9420794248580933, Train: 0.1571, Val: 0.1680, Test: 0.1920\n",
      "Epoch: 094, Loss: 1.9414517879486084, Train: 0.1571, Val: 0.1720, Test: 0.1920\n",
      "Epoch: 095, Loss: 1.9408224821090698, Train: 0.1571, Val: 0.1720, Test: 0.1920\n",
      "Epoch: 096, Loss: 1.9402003288269043, Train: 0.1571, Val: 0.1740, Test: 0.1920\n",
      "Epoch: 097, Loss: 1.939573884010315, Train: 0.1571, Val: 0.1780, Test: 0.1920\n",
      "Epoch: 098, Loss: 1.9389480352401733, Train: 0.1571, Val: 0.1780, Test: 0.1920\n",
      "Epoch: 099, Loss: 1.9383093118667603, Train: 0.1500, Val: 0.1800, Test: 0.1920\n",
      "Epoch: 100, Loss: 1.937678575515747, Train: 0.1500, Val: 0.1800, Test: 0.1920\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you can train the GCN model with PyG. Next, you may try using the DGL to implement the similiar function."
   ],
   "metadata": {
    "id": "4pav74NceOl5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import AddSelfLoop\n",
    "# data bug ,the reason is the 0cloud, the host is wrong, turn off it.\n",
    "from dgl.data import CoraGraphDataset\n",
    "\n",
    "\n",
    "transform = (\n",
    "        AddSelfLoop()\n",
    "    )\n",
    "data = CoraGraphDataset(transform=transform)\n",
    "g = data[0]\n",
    "\n",
    "features = g.ndata[\"feat\"]\n",
    "labels = g.ndata[\"label\"]\n",
    "masks = g.ndata[\"train_mask\"], g.ndata[\"val_mask\"], g.ndata[\"test_mask\"]\n",
    "# edge_weight = g.ndata['edata']\n",
    "\n",
    "class DGL_conv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(DGL_conv, self).__init__()\n",
    "        self.input_channel = input_channels\n",
    "        self.output_channel = output_channels\n",
    "        self.W = nn.Parameter(torch.ones((input_channels,output_channels)))\n",
    "        self.b = nn.Parameter(torch.ones(output_channels))\n",
    "    def forward (self, g, h):\n",
    "        with g.local_scope(): # local_scope 函数的目的是在这个区域进行信息传递，聚合和更新，但是并不修改原图的节点或边的特征\n",
    "            # g.edata['w'] = edge_weight\n",
    "            h = torch.matmul(h, self.W)\n",
    "            # h = torch.matmul(h,g.edges['w'])\n",
    "            g.ndata['h'] = h\n",
    "            # g.update_all(dgl.function.u_mul_e('h','w', 'm'), dgl.function.sum('m', 'h_sum'))\n",
    "            #如果没有边，则直接用节点的信息根据边的连接情况进行消息传递\n",
    "            g.update_all(dgl.function.copy_u('h', 'm'), dgl.function.sum('m', 'h_sum'))\n",
    "            h_sum = g.ndata['h_sum']\n",
    "            h_message = h_sum + self.b\n",
    "            return h_message\n",
    "\n",
    "class DGL_GCN(nn.Module):\n",
    "  # Your code here\n",
    "  def __init__(self, in_channels, out_channels, hidden_dim):\n",
    "      super(DGL_GCN,self).__init__()\n",
    "      self.conv1 = DGL_conv(in_channels, hidden_dim)\n",
    "      self.conv2 = DGL_conv(hidden_dim, out_channels)\n",
    "  def forward(self,g, h):\n",
    "  \n",
    "      #经过每层网络\n",
    "      x = self.conv1(g, h)\n",
    "      x = F.relu(x)\n",
    "      x = F.dropout(x, training = self.training)\n",
    "      x = self.conv2(g, x)\n",
    "      return F.log_softmax(x,dim=1)    \n",
    "      \n",
    "  # End code here\n",
    "\n",
    "def train(g, features, labels, masks, model):\n",
    "  # Your code here\n",
    "  train_mask = masks[0]\n",
    "  print('---------',train_mask,len(train_mask))\n",
    "  \n",
    "  val_mask = masks[1]\n",
    "  print('---------',val_mask,len(val_mask))\n",
    "  loss_fcn = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "  \n",
    "  for epoch in range(200):\n",
    "      model.train()\n",
    "      logits = model(g,features)\n",
    "      loss = loss_fcn(logits[train_mask],labels[train_mask])\n",
    "      optimizer.zero_grad()\n",
    "      # out = model(g, features)\n",
    "      # loss = F.nll_loss(out[data.train_mask],data.y[data.train_mask])\n",
    "      #反向传播,计算梯度\n",
    "      loss.backward()\n",
    "      #用于根据上一步计算的梯度更新模型的参数，以最小化损失函数\n",
    "      optimizer.step()\n",
    "      acc = evaluate(g, features, labels, val_mask, model)\n",
    "      print(\n",
    "            \"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} \".format(\n",
    "                epoch, loss.item(), acc\n",
    "            )\n",
    "        )\n",
    "  # End code here\n",
    "\n",
    "def evaluate(g, features, labels, mask, model):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    logits = model(g, features)\n",
    "    logits = logits[mask]\n",
    "    labels = labels[mask]\n",
    "    _, indices = torch.max(logits, dim=1)\n",
    "    correct = torch.sum(indices == labels)\n",
    "    return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "model = DGL_GCN(features.shape[1],data.num_classes, hidden_dim)\n",
    "print(\"Training...\")\n",
    "train(g, features, labels, masks, model)\n",
    "\n",
    "# test the model\n",
    "print(\"Testing...\")\n",
    "acc = evaluate(g, features, labels, masks[2], model)\n",
    "print(\"Test accuracy {:.4f}\".format(acc))"
   ],
   "metadata": {
    "id": "eNDS5VDreXgC",
    "ExecuteTime": {
     "end_time": "2024-07-04T05:26:04.629212200Z",
     "start_time": "2024-07-04T05:26:00.891415300Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Training...\n",
      "--------- tensor([ True,  True,  True,  ..., False, False, False]) 2708\n",
      "--------- tensor([False, False, False,  ..., False, False, False]) 2708\n",
      "Epoch 00000 | Loss 1.9459 | Accuracy 0.1560 \n",
      "Epoch 00001 | Loss 5.3192 | Accuracy 0.0720 \n",
      "Epoch 00002 | Loss 8.2273 | Accuracy 0.1220 \n",
      "Epoch 00003 | Loss 11.9271 | Accuracy 0.1620 \n",
      "Epoch 00004 | Loss 7.9609 | Accuracy 0.0580 \n",
      "Epoch 00005 | Loss 4.9593 | Accuracy 0.1140 \n",
      "Epoch 00006 | Loss 8.5841 | Accuracy 0.1560 \n",
      "Epoch 00007 | Loss 7.0817 | Accuracy 0.1560 \n",
      "Epoch 00008 | Loss 7.5332 | Accuracy 0.3160 \n",
      "Epoch 00009 | Loss 5.2327 | Accuracy 0.0720 \n",
      "Epoch 00010 | Loss 6.4476 | Accuracy 0.0720 \n",
      "Epoch 00011 | Loss 7.2973 | Accuracy 0.0720 \n",
      "Epoch 00012 | Loss 5.2788 | Accuracy 0.0720 \n",
      "Epoch 00013 | Loss 5.2488 | Accuracy 0.1220 \n",
      "Epoch 00014 | Loss 3.4337 | Accuracy 0.1220 \n",
      "Epoch 00015 | Loss 5.4829 | Accuracy 0.3140 \n",
      "Epoch 00016 | Loss 5.4687 | Accuracy 0.1620 \n",
      "Epoch 00017 | Loss 5.0449 | Accuracy 0.1620 \n",
      "Epoch 00018 | Loss 5.5196 | Accuracy 0.3160 \n",
      "Epoch 00019 | Loss 4.9504 | Accuracy 0.2120 \n",
      "Epoch 00020 | Loss 4.1546 | Accuracy 0.1140 \n",
      "Epoch 00021 | Loss 3.5903 | Accuracy 0.0580 \n",
      "Epoch 00022 | Loss 3.7084 | Accuracy 0.0720 \n",
      "Epoch 00023 | Loss 3.1280 | Accuracy 0.0720 \n",
      "Epoch 00024 | Loss 3.1331 | Accuracy 0.0720 \n",
      "Epoch 00025 | Loss 2.8864 | Accuracy 0.1120 \n",
      "Epoch 00026 | Loss 4.2726 | Accuracy 0.2000 \n",
      "Epoch 00027 | Loss 4.0803 | Accuracy 0.1220 \n",
      "Epoch 00028 | Loss 4.9303 | Accuracy 0.1620 \n",
      "Epoch 00029 | Loss 3.0735 | Accuracy 0.1620 \n",
      "Epoch 00030 | Loss 3.7953 | Accuracy 0.1640 \n",
      "Epoch 00031 | Loss 2.9925 | Accuracy 0.1060 \n",
      "Epoch 00032 | Loss 4.3830 | Accuracy 0.2540 \n",
      "Epoch 00033 | Loss 3.3866 | Accuracy 0.3460 \n",
      "Epoch 00034 | Loss 3.5534 | Accuracy 0.3020 \n",
      "Epoch 00035 | Loss 4.6991 | Accuracy 0.3020 \n",
      "Epoch 00036 | Loss 3.6380 | Accuracy 0.0960 \n",
      "Epoch 00037 | Loss 3.8645 | Accuracy 0.0980 \n",
      "Epoch 00038 | Loss 3.0784 | Accuracy 0.1120 \n",
      "Epoch 00039 | Loss 4.0026 | Accuracy 0.2040 \n",
      "Epoch 00040 | Loss 3.5800 | Accuracy 0.1580 \n",
      "Epoch 00041 | Loss 4.2854 | Accuracy 0.2500 \n",
      "Epoch 00042 | Loss 4.4896 | Accuracy 0.2760 \n",
      "Epoch 00043 | Loss 2.9249 | Accuracy 0.2920 \n",
      "Epoch 00044 | Loss 4.1906 | Accuracy 0.2740 \n",
      "Epoch 00045 | Loss 3.6217 | Accuracy 0.2520 \n",
      "Epoch 00046 | Loss 2.4141 | Accuracy 0.0900 \n",
      "Epoch 00047 | Loss 2.8660 | Accuracy 0.2000 \n",
      "Epoch 00048 | Loss 2.8426 | Accuracy 0.2680 \n",
      "Epoch 00049 | Loss 2.8913 | Accuracy 0.1620 \n",
      "Epoch 00050 | Loss 2.6279 | Accuracy 0.1800 \n",
      "Epoch 00051 | Loss 4.0320 | Accuracy 0.2040 \n",
      "Epoch 00052 | Loss 2.5478 | Accuracy 0.2320 \n",
      "Epoch 00053 | Loss 3.0407 | Accuracy 0.2400 \n",
      "Epoch 00054 | Loss 2.0989 | Accuracy 0.1760 \n",
      "Epoch 00055 | Loss 2.7063 | Accuracy 0.1220 \n",
      "Epoch 00056 | Loss 2.4206 | Accuracy 0.2220 \n",
      "Epoch 00057 | Loss 2.0908 | Accuracy 0.2180 \n",
      "Epoch 00058 | Loss 3.1735 | Accuracy 0.1620 \n",
      "Epoch 00059 | Loss 2.7652 | Accuracy 0.1720 \n",
      "Epoch 00060 | Loss 2.6395 | Accuracy 0.2220 \n",
      "Epoch 00061 | Loss 3.1289 | Accuracy 0.1620 \n",
      "Epoch 00062 | Loss 2.8076 | Accuracy 0.1700 \n",
      "Epoch 00063 | Loss 2.3864 | Accuracy 0.2360 \n",
      "Epoch 00064 | Loss 3.3795 | Accuracy 0.2120 \n",
      "Epoch 00065 | Loss 2.9378 | Accuracy 0.1340 \n",
      "Epoch 00066 | Loss 3.2709 | Accuracy 0.0920 \n",
      "Epoch 00067 | Loss 2.4159 | Accuracy 0.0880 \n",
      "Epoch 00068 | Loss 3.0366 | Accuracy 0.1660 \n",
      "Epoch 00069 | Loss 2.6858 | Accuracy 0.1660 \n",
      "Epoch 00070 | Loss 2.5987 | Accuracy 0.1940 \n",
      "Epoch 00071 | Loss 2.3422 | Accuracy 0.1600 \n",
      "Epoch 00072 | Loss 2.2283 | Accuracy 0.1620 \n",
      "Epoch 00073 | Loss 2.1346 | Accuracy 0.2860 \n",
      "Epoch 00074 | Loss 2.0806 | Accuracy 0.2720 \n",
      "Epoch 00075 | Loss 2.1917 | Accuracy 0.1020 \n",
      "Epoch 00076 | Loss 2.0479 | Accuracy 0.1000 \n",
      "Epoch 00077 | Loss 2.0942 | Accuracy 0.1000 \n",
      "Epoch 00078 | Loss 2.5259 | Accuracy 0.0920 \n",
      "Epoch 00079 | Loss 3.3958 | Accuracy 0.1660 \n",
      "Epoch 00080 | Loss 2.6183 | Accuracy 0.2380 \n",
      "Epoch 00081 | Loss 2.2115 | Accuracy 0.1680 \n",
      "Epoch 00082 | Loss 2.7179 | Accuracy 0.1880 \n",
      "Epoch 00083 | Loss 2.3115 | Accuracy 0.2760 \n",
      "Epoch 00084 | Loss 2.8685 | Accuracy 0.2160 \n",
      "Epoch 00085 | Loss 2.3974 | Accuracy 0.1520 \n",
      "Epoch 00086 | Loss 2.1719 | Accuracy 0.2060 \n",
      "Epoch 00087 | Loss 3.1793 | Accuracy 0.1080 \n",
      "Epoch 00088 | Loss 2.0904 | Accuracy 0.1020 \n",
      "Epoch 00089 | Loss 3.0711 | Accuracy 0.1200 \n",
      "Epoch 00090 | Loss 2.0522 | Accuracy 0.1240 \n",
      "Epoch 00091 | Loss 2.5248 | Accuracy 0.1900 \n",
      "Epoch 00092 | Loss 2.2736 | Accuracy 0.2160 \n",
      "Epoch 00093 | Loss 2.6164 | Accuracy 0.1700 \n",
      "Epoch 00094 | Loss 2.2878 | Accuracy 0.1760 \n",
      "Epoch 00095 | Loss 2.6643 | Accuracy 0.1880 \n",
      "Epoch 00096 | Loss 2.6512 | Accuracy 0.1940 \n",
      "Epoch 00097 | Loss 2.6374 | Accuracy 0.2400 \n",
      "Epoch 00098 | Loss 2.6047 | Accuracy 0.2640 \n",
      "Epoch 00099 | Loss 2.0775 | Accuracy 0.2160 \n",
      "Epoch 00100 | Loss 2.3328 | Accuracy 0.2720 \n",
      "Epoch 00101 | Loss 1.9269 | Accuracy 0.2720 \n",
      "Epoch 00102 | Loss 1.7567 | Accuracy 0.2900 \n",
      "Epoch 00103 | Loss 1.9423 | Accuracy 0.1280 \n",
      "Epoch 00104 | Loss 2.4085 | Accuracy 0.1120 \n",
      "Epoch 00105 | Loss 2.0937 | Accuracy 0.1220 \n",
      "Epoch 00106 | Loss 2.1091 | Accuracy 0.1160 \n",
      "Epoch 00107 | Loss 1.9380 | Accuracy 0.1840 \n",
      "Epoch 00108 | Loss 1.8930 | Accuracy 0.2620 \n",
      "Epoch 00109 | Loss 1.8881 | Accuracy 0.2960 \n",
      "Epoch 00110 | Loss 1.7720 | Accuracy 0.3740 \n",
      "Epoch 00111 | Loss 2.4183 | Accuracy 0.3020 \n",
      "Epoch 00112 | Loss 2.4614 | Accuracy 0.2920 \n",
      "Epoch 00113 | Loss 2.2946 | Accuracy 0.2760 \n",
      "Epoch 00114 | Loss 1.8491 | Accuracy 0.2600 \n",
      "Epoch 00115 | Loss 1.9717 | Accuracy 0.2720 \n",
      "Epoch 00116 | Loss 1.7749 | Accuracy 0.2660 \n",
      "Epoch 00117 | Loss 1.6899 | Accuracy 0.2240 \n",
      "Epoch 00118 | Loss 1.9033 | Accuracy 0.1700 \n",
      "Epoch 00119 | Loss 1.7831 | Accuracy 0.2460 \n",
      "Epoch 00120 | Loss 2.0920 | Accuracy 0.2120 \n",
      "Epoch 00121 | Loss 1.8356 | Accuracy 0.1180 \n",
      "Epoch 00122 | Loss 1.7201 | Accuracy 0.1160 \n",
      "Epoch 00123 | Loss 2.1791 | Accuracy 0.1360 \n",
      "Epoch 00124 | Loss 1.8168 | Accuracy 0.1980 \n",
      "Epoch 00125 | Loss 1.8602 | Accuracy 0.2580 \n",
      "Epoch 00126 | Loss 2.4372 | Accuracy 0.2900 \n",
      "Epoch 00127 | Loss 2.0656 | Accuracy 0.2520 \n",
      "Epoch 00128 | Loss 1.8402 | Accuracy 0.2360 \n",
      "Epoch 00129 | Loss 1.9186 | Accuracy 0.2360 \n",
      "Epoch 00130 | Loss 2.1019 | Accuracy 0.3560 \n",
      "Epoch 00131 | Loss 1.7638 | Accuracy 0.4160 \n",
      "Epoch 00132 | Loss 1.8955 | Accuracy 0.3960 \n",
      "Epoch 00133 | Loss 1.7856 | Accuracy 0.3940 \n",
      "Epoch 00134 | Loss 1.7070 | Accuracy 0.2440 \n",
      "Epoch 00135 | Loss 2.1727 | Accuracy 0.2340 \n",
      "Epoch 00136 | Loss 1.8591 | Accuracy 0.2940 \n",
      "Epoch 00137 | Loss 2.2250 | Accuracy 0.2780 \n",
      "Epoch 00138 | Loss 1.5398 | Accuracy 0.2560 \n",
      "Epoch 00139 | Loss 1.6226 | Accuracy 0.2840 \n",
      "Epoch 00140 | Loss 1.6610 | Accuracy 0.3460 \n",
      "Epoch 00141 | Loss 1.7697 | Accuracy 0.3620 \n",
      "Epoch 00142 | Loss 2.0019 | Accuracy 0.3880 \n",
      "Epoch 00143 | Loss 1.8310 | Accuracy 0.2940 \n",
      "Epoch 00144 | Loss 1.6584 | Accuracy 0.2940 \n",
      "Epoch 00145 | Loss 1.7982 | Accuracy 0.3000 \n",
      "Epoch 00146 | Loss 1.8076 | Accuracy 0.3000 \n",
      "Epoch 00147 | Loss 1.7197 | Accuracy 0.3000 \n",
      "Epoch 00148 | Loss 1.7775 | Accuracy 0.3040 \n",
      "Epoch 00149 | Loss 1.6241 | Accuracy 0.3140 \n",
      "Epoch 00150 | Loss 1.6757 | Accuracy 0.3000 \n",
      "Epoch 00151 | Loss 1.6199 | Accuracy 0.3180 \n",
      "Epoch 00152 | Loss 1.6394 | Accuracy 0.3900 \n",
      "Epoch 00153 | Loss 1.5556 | Accuracy 0.4460 \n",
      "Epoch 00154 | Loss 1.6361 | Accuracy 0.3860 \n",
      "Epoch 00155 | Loss 1.7435 | Accuracy 0.3280 \n",
      "Epoch 00156 | Loss 1.8560 | Accuracy 0.3340 \n",
      "Epoch 00157 | Loss 2.2480 | Accuracy 0.3860 \n",
      "Epoch 00158 | Loss 1.8823 | Accuracy 0.3320 \n",
      "Epoch 00159 | Loss 1.4421 | Accuracy 0.3240 \n",
      "Epoch 00160 | Loss 1.5519 | Accuracy 0.3600 \n",
      "Epoch 00161 | Loss 1.5758 | Accuracy 0.3800 \n",
      "Epoch 00162 | Loss 1.4870 | Accuracy 0.3740 \n",
      "Epoch 00163 | Loss 1.5805 | Accuracy 0.3740 \n",
      "Epoch 00164 | Loss 1.6009 | Accuracy 0.3760 \n",
      "Epoch 00165 | Loss 1.5332 | Accuracy 0.4160 \n",
      "Epoch 00166 | Loss 1.4606 | Accuracy 0.4720 \n",
      "Epoch 00167 | Loss 1.5223 | Accuracy 0.5140 \n",
      "Epoch 00168 | Loss 1.4760 | Accuracy 0.5160 \n",
      "Epoch 00169 | Loss 1.4974 | Accuracy 0.5240 \n",
      "Epoch 00170 | Loss 1.4904 | Accuracy 0.4860 \n",
      "Epoch 00171 | Loss 1.4869 | Accuracy 0.4720 \n",
      "Epoch 00172 | Loss 1.5044 | Accuracy 0.4800 \n",
      "Epoch 00173 | Loss 1.4253 | Accuracy 0.4840 \n",
      "Epoch 00174 | Loss 1.4631 | Accuracy 0.5060 \n",
      "Epoch 00175 | Loss 1.8407 | Accuracy 0.5100 \n",
      "Epoch 00176 | Loss 1.3851 | Accuracy 0.4560 \n",
      "Epoch 00177 | Loss 1.4700 | Accuracy 0.4040 \n",
      "Epoch 00178 | Loss 1.4494 | Accuracy 0.4000 \n",
      "Epoch 00179 | Loss 1.4265 | Accuracy 0.3940 \n",
      "Epoch 00180 | Loss 1.6360 | Accuracy 0.4000 \n",
      "Epoch 00181 | Loss 1.4838 | Accuracy 0.4060 \n",
      "Epoch 00182 | Loss 1.4532 | Accuracy 0.3940 \n",
      "Epoch 00183 | Loss 1.4037 | Accuracy 0.4180 \n",
      "Epoch 00184 | Loss 1.4457 | Accuracy 0.4720 \n",
      "Epoch 00185 | Loss 1.4064 | Accuracy 0.4960 \n",
      "Epoch 00186 | Loss 1.4560 | Accuracy 0.5060 \n",
      "Epoch 00187 | Loss 1.4115 | Accuracy 0.5240 \n",
      "Epoch 00188 | Loss 1.3991 | Accuracy 0.5340 \n",
      "Epoch 00189 | Loss 1.3458 | Accuracy 0.5520 \n",
      "Epoch 00190 | Loss 1.3916 | Accuracy 0.5520 \n",
      "Epoch 00191 | Loss 1.4160 | Accuracy 0.5580 \n",
      "Epoch 00192 | Loss 1.3253 | Accuracy 0.5600 \n",
      "Epoch 00193 | Loss 1.3601 | Accuracy 0.6040 \n",
      "Epoch 00194 | Loss 1.3179 | Accuracy 0.5880 \n",
      "Epoch 00195 | Loss 1.2637 | Accuracy 0.5480 \n",
      "Epoch 00196 | Loss 1.4441 | Accuracy 0.5220 \n",
      "Epoch 00197 | Loss 1.2999 | Accuracy 0.5080 \n",
      "Epoch 00198 | Loss 1.4135 | Accuracy 0.5060 \n",
      "Epoch 00199 | Loss 1.2291 | Accuracy 0.5100 \n",
      "Testing...\n",
      "Test accuracy 0.4730\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you find it hard to implement, you may refer to the official implementation of the GNN training, like [PyG](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py) and [DGL](https://github.com/dmlc/dgl/blob/master/examples/pytorch/gcn/train.py)."
   ],
   "metadata": {
    "id": "nG3acaBaf0Uj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T05:26:04.664220900Z",
     "start_time": "2024-07-04T05:26:04.608208Z"
    }
   }
  }
 ]
}
